Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Argall2009,
abstract = {We present a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Argall, Brenna D. and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
doi = {10.1016/j.robot.2008.10.024},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Argall et al. - 2009 - A survey of robot learning from demonstration.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Autonomous systems,Learning from demonstration,Machine learning,Robotics},
number = {5},
pages = {469--483},
title = {{A survey of robot learning from demonstration}},
volume = {57},
year = {2009}
}
@article{Arora2018,
abstract = {Inverse reinforcement learning is the problem of inferring the reward function of an observed agent, given its policy or behavior. Researchers perceive IRL both as a problem and as a class of methods. By categorically surveying the current literature in IRL, this article serves as a reference for researchers and practitioners in machine learning to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges which include accurate inference, generalizability, correctness of prior knowledge, and growth in solution complexity with problem size. The article elaborates how the current methods mitigate these challenges. We further discuss the extensions of traditional IRL methods: (i) inaccurate and incomplete perception, (ii) incomplete model, (iii) multiple rewards, and (iv) non-linear reward functions. This discussion concludes with some broad advances in the research area and currently open research questions.},
archivePrefix = {arXiv},
arxivId = {1806.06877},
author = {Arora, Saurabh and Doshi, Prashant},
eprint = {1806.06877},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Arora, Doshi - 2018 - A Survey of Inverse Reinforcement Learning Challenges, Methods and Progress.pdf:pdf},
keywords = {cs,edu,email addresses,function,generalization,imitation learning,inverse reinforcement learning,learning from demonstration,pdoshi,prashant,reinforcement learning,reward,sa08751,saurabh arora,uga},
title = {{A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress}},
url = {http://arxiv.org/abs/1806.06877},
year = {2018}
}
@article{Mousavi2018a,
abstract = {In recent years, a specific machine learning method called deep learning has gained huge attraction, as it has obtained astonishing results in broad applications such as pattern recognition, speech recognition, computer vision, and natural language processing. Recent research has also been shown that deep learning techniques can be combined with reinforcement learning methods to learn useful representations for the problems with high dimensional raw data input. This chapter reviews the recent advances in deep reinforcement learning with a focus on the most used deep architectures such as autoencoders, convolutional neural networks and recurrent neural networks which have successfully been come together with the reinforcement learning framework.},
archivePrefix = {arXiv},
arxivId = {arXiv:1701.07274v6},
author = {Mousavi, Seyed Sajad and Schukat, Michael and Howley, Enda},
doi = {10.1007/978-3-319-56991-8_32},
eprint = {arXiv:1701.07274v6},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop//Mousavi, Schukat, Howley - 2018 - Deep Reinforcement Learning An Overview.pdf:pdf},
issn = {23673389},
journal = {Lecture Notes in Networks and Systems},
keywords = {Deep leaning,MDPs,Neural networks,Observable MDPs,Reinforcement learning},
pages = {426--440},
title = {{Deep Reinforcement Learning: An Overview}},
volume = {16},
year = {2018}
}
@article{Abel2019a,
author = {Abel, David},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop//Beach, Abel - 2019 - ICML 2019 Notes.pdf:pdf},
number = {June},
pages = {1--87},
title = {{ICML 2019 Notes}},
year = {2019}
}
