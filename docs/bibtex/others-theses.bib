Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Cruz2017,
abstract = {Intelligent assistive robots have recently taken their rst steps toward entering domestic scenarios. It is thus expected that they perform tasks which are often considered rather simple for humans. However, for a robot to reach human-like performance diverse subtasks need to be accomplished in order to satisfactorily complete a given task. These subtasks include perception, understanding of the environment, learning strategies, knowledge representation, awareness of its own state, and manipulation of the environment. An open challenging issue is the time required by a robot to autonomously learn a new task. A strategy to speed up this apprenticeship period for autonomous robots is the integration of parent-like trainers to sca old the learning. In this regard, a trainer guides the robot to enhance the task performance in the same manner as caregivers may support infants in the accomplishment of a given task. In this thesis, we focus on these learning approaches, speci cally on interactive reinforcement learning to perform a domestic task. We use parent-like advice to explore two set-ups: agent-agent and human-agent interaction. First, we investigate agent-agent interactive reinforcement learning. We use an arti cial agent as a parent-like trainer. The arti cial agent is previously trained by autonomous reinforcement learning and afterward becomes the trainer of other agents. This interactive scenario allows us to experiment with the interplay of parameters like the probability of receiving feedback and the consistency of feedback. We show that the consistency of feedback deserves special attention since small variations on this parameter may considerably affect the learner's performance. Moreover, we introduce the concept of contextual affordances which allows to reduce the state-action space by avoiding failed-states, i.e., to avoid a group of states from which it is not possible to reach the goal-state of a task. By avoiding failed-states, the learner-agent is able to collect signi cantly more reward. The experiments also focus on the internal representation of knowledge in trainer-agents to improve the understanding of what the properties of a good teacher are. We show that using a polymath agent, i.e., an agent with more distributed knowledge among the states, it is possible to o er better advice to learner-agents compared to specialized agents. Thereafter, we study human-agent interactive reinforcement learning. Initially, experiments are performed with human parent-like advice using uni-modal speech guidance. The experimental set-up considers the use of di erent auditory sensors to compare how they a ect the consistency of advice and the learning performance. We observe that an impoverished speech recognition system may still help interactive reinforcement learning agents, although not to the same extent as in the ideal case of agent-agent interaction. Afterward, we perform an experiment including audiovisual parent-like advice. The set-up takes into account the integration of multi-modal cues in order to combine them into a single piece of consistent advice for the learner-agent. Additionally, we utilize contextual affordances to modulate the advice given to the robot to avoid failed-states and to effectively speed up the learning process. Multi-modal feedback produces more con dent levels of advice allowing learner-agents to bene t from this in order to obtain more reward and to gain it faster. This thesis contributes to knowledge in terms of studying the interplay of multimodal interactive feedback and contextual affordances. Overall, we investigate which parameters in uence the interactive reinforcement learning process and show that the apprenticeship of reinforcement learning agents can be sped up by means of interactive parent-like advice, multi-modal feedback, and a ordances-driven environmental models.},
author = {Cruz, Francisco},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Dissertation.pdf:pdf},
keywords = {Contextual affordances,Multi-modal integration,Reinforcement learning},
pages = {169},
title = {{Teaching Robots With Interactive Reinforcement Learning}},
year = {2017}
}
@article{Sosic2018,
author = {{\v{S}}o{\v{s}}i{\'{c}}, Adrian and Sc, M},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/{\v{S}}o{\v{s}}i{\'{c}}, Sc - 2018 - Learning Models of Behavior From Demonstration and Through Interaction.pdf:pdf},
title = {{Learning Models of Behavior From Demonstration and Through Interaction}},
year = {2018}
}
@article{Nørstebø2016,
author = {N{\o}rsteb{\o}, Olav and Bjertnes, Vegard R{\o}dseth and Vabo, Eirik},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/N{\o}rsteb{\o}, Bjertnes, Vabo - 2016 - Valuing Individual Player Involvements in Norwegian Association Football Master's Thesis.pdf:pdf},
pages = {126},
title = {{Valuing Individual Player Involvements in Norwegian Association Football [Master's Thesis]}},
year = {2016}
}
