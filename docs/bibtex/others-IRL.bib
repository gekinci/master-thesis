Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Thanh2016,
abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods. We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories. Copyright {\textcopyright} 2008.},
author = {Thanh, Ho Vinh and An, Le Thi Hoai and Chien, Bui Dinh},
doi = {10.1007/978-3-662-49390-8_64},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Thanh, An, Chien - 2016 - Maximum Entropy Inverse Reinforcement Learning.pdf:pdf},
isbn = {9783662493892},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {DC programming,DCA,Online DC optimization,Online DCA,Online binary classification,Perceptron},
pages = {661--670},
title = {{Maximum Entropy Inverse Reinforcement Learning}},
volume = {9622},
year = {2016}
}
@article{Alger2016,
abstract = {In this report, we describe " inverse reinforcement learning " . This is the problem of finding an unknown reward function for a Markov decision process (MDP), given an optimal policy for that MDP. We also describe existing methods to solve this problem, and investigate a pre-existing deep learning extension to these methods.},
author = {Alger, Maahew},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/irl.pdf:pdf},
pages = {1--19},
title = {{Deep Inverse Reinforcement Learning}},
url = {https://matthewja.com/pdfs/irl.pdf},
year = {2016}
}
@article{Wang2018a,
abstract = {This paper considers the problem of inverse reinforcement learning in zero-sum stochastic games when expert demonstrations are known to be not optimal. Compared to previous works that decouple agents in the game by assuming optimality in expert strategies, we introduce a new objective function that directly pits experts against Nash Equilibrium strategies, and we design an algorithm to solve for the reward function in the context of inverse reinforcement learning with deep neural networks as model approximations. In our setting the model and algorithm do not decouple by agent. In order to find Nash Equilibrium in large-scale games, we also propose an adversarial training algorithm for zero-sum stochastic games, and show the theoretical appeal of non-existence of local optima in its objective function. In our numerical experiments, we demonstrate that our Nash Equilibrium and inverse reinforcement learning algorithms address games that are not amenable to previous approaches using tabular representations. Moreover, with sub-optimal expert demonstrations our algorithms recover both reward functions and strategies with good quality.},
archivePrefix = {arXiv},
arxivId = {1801.02124},
author = {Wang, Xingyu and Klabjan, Diego},
eprint = {1801.02124},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Wang, Klabjan - 2018 - Competitive Multi-agent Inverse Reinforcement Learning with Sub-optimal Demonstrations.pdf:pdf},
title = {{Competitive Multi-agent Inverse Reinforcement Learning with Sub-optimal Demonstrations}},
url = {http://arxiv.org/abs/1801.02124},
year = {2018}
}
@article{Hadfield-Menell2016,
abstract = {For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.},
archivePrefix = {arXiv},
arxivId = {1606.03137},
author = {Hadfield-Menell, Dylan and Dragan, Anca and Abbeel, Pieter and Russell, Stuart},
eprint = {1606.03137},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/1606.03137v2.pdf:pdf},
title = {{Cooperative Inverse Reinforcement Learning}},
url = {http://arxiv.org/abs/1606.03137},
year = {2016}
}
@article{Brown2018,
abstract = {Inverse reinforcement learning (IRL) infers a reward function from demonstrations, allowing for policy improvement and generalization. However, despite much recent interest in IRL, little work has been done to understand the minimum set of demonstrations needed to teach a specific sequential decision-making task. We formalize the problem of finding maximally informative demonstrations for IRL as a machine teaching problem where the goal is to find the minimum number of demonstrations needed to specify the reward equivalence class of the demonstrator. We extend previous work on algorithmic teaching for sequential decision-making tasks by showing a reduction to the set cover problem which enables an efficient approximation algorithm for determining the set of maximally-informative demonstrations. We apply our proposed machine teaching algorithm to two novel applications: providing a lower bound on the number of queries needed to learn a policy using active IRL and developing a novel IRL algorithm that can learn more efficiently from informative demonstrations than a standard IRL approach.},
archivePrefix = {arXiv},
arxivId = {1805.07687},
author = {Brown, Daniel S. and Niekum, Scott},
eprint = {1805.07687},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Brown, Niekum - 2018 - Machine Teaching for Inverse Reinforcement Learning Algorithms and Applications.pdf:pdf},
number = {2014},
title = {{Machine Teaching for Inverse Reinforcement Learning: Algorithms and Applications}},
url = {http://arxiv.org/abs/1805.07687},
year = {2018}
}
@article{Arora2018,
abstract = {Inverse reinforcement learning is the problem of inferring the reward function of an observed agent, given its policy or behavior. Researchers perceive IRL both as a problem and as a class of methods. By categorically surveying the current literature in IRL, this article serves as a reference for researchers and practitioners in machine learning to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges which include accurate inference, generalizability, correctness of prior knowledge, and growth in solution complexity with problem size. The article elaborates how the current methods mitigate these challenges. We further discuss the extensions of traditional IRL methods: (i) inaccurate and incomplete perception, (ii) incomplete model, (iii) multiple rewards, and (iv) non-linear reward functions. This discussion concludes with some broad advances in the research area and currently open research questions.},
archivePrefix = {arXiv},
arxivId = {1806.06877},
author = {Arora, Saurabh and Doshi, Prashant},
eprint = {1806.06877},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Arora, Doshi - 2018 - A Survey of Inverse Reinforcement Learning Challenges, Methods and Progress.pdf:pdf},
keywords = {cs,edu,email addresses,function,generalization,imitation learning,inverse reinforcement learning,learning from demonstration,pdoshi,prashant,reinforcement learning,reward,sa08751,saurabh arora,uga},
title = {{A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress}},
url = {http://arxiv.org/abs/1806.06877},
year = {2018}
}
@article{Abbeel2004,
abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our. algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
author = {Abbeel, Pieter and Ng, Andrew Y.},
doi = {10.1145/1015330.1015430},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop//Microcentrifuge - 2011 - Sorvall Legend Micro 21R.pdf:pdf},
isbn = {1581138285},
journal = {Twenty-first international conference on Machine learning  - ICML '04},
number = {346},
pages = {1},
title = {{Apprenticeship learning via inverse reinforcement learning}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015430},
year = {2004}
}
