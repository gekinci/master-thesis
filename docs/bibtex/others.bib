Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Keskar2016,
abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say {\$}32{\$}-{\$}512{\$} data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
archivePrefix = {arXiv},
arxivId = {1609.04836},
author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
eprint = {1609.04836},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Keskar et al. - 2016 - On Large-Batch Training for Deep Learning Generalization Gap and Sharp Minima.pdf:pdf},
pages = {1--16},
title = {{On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}},
url = {http://arxiv.org/abs/1609.04836},
year = {2016}
}
@article{Duan2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.02779v2},
author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter and Science, Computer},
eprint = {arXiv:1611.02779v2},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Duan et al. - 2017 - R EINFORCEMENT L EARNING.pdf:pdf},
pages = {1--14},
title = {{RL2: FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING}},
year = {2017}
}
@article{Thanh2016,
abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods. We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories. Copyright {\textcopyright} 2008.},
author = {Thanh, Ho Vinh and An, Le Thi Hoai and Chien, Bui Dinh},
doi = {10.1007/978-3-662-49390-8_64},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Thanh, An, Chien - 2016 - Maximum Entropy Inverse Reinforcement Learning.pdf:pdf},
isbn = {9783662493892},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {DC programming,DCA,Online DC optimization,Online DCA,Online binary classification,Perceptron},
pages = {661--670},
title = {{Maximum Entropy Inverse Reinforcement Learning}},
volume = {9622},
year = {2016}
}
@article{Chan2019,
abstract = {Learning preferences implicit in the choices humans make is a well studied problem in both economics and computer science. However, most work makes the assumption that humans are acting (noisily) optimally with respect to their preferences. Such approaches can fail when people are themselves learning about what they want. In this work, we introduce the assistive multi-armed bandit, where a robot assists a human playing a bandit task to maximize cumulative reward. In this problem, the human does not know the reward function but can learn it through the rewards received from arm pulls; the robot only observes which arms the human pulls but not the reward associated with each pull. We offer sufficient and necessary conditions for successfully assisting the human in this framework. Surprisingly, better human performance in isolation does not necessarily lead to better performance when assisted by the robot: a human policy can do better by effectively communicating its observed rewards to the robot. We conduct proof-of-concept experiments that support these results. We see this work as contributing towards a theory behind algorithms for human-robot interaction.},
archivePrefix = {arXiv},
arxivId = {arXiv:1901.08654v1},
author = {Chan, Lawrence and Hadfield-Menell, Dylan and Srinivasa, Siddhartha and Dragan, Anca},
doi = {10.1109/HRI.2019.8673234},
eprint = {arXiv:1901.08654v1},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/1901.08654.pdf:pdf},
isbn = {9781538685556},
issn = {21672148},
journal = {ACM/IEEE International Conference on Human-Robot Interaction},
keywords = {assistive agents,preference learning},
number = {Figure 1},
pages = {354--363},
title = {{The Assistive Multi-Armed Bandit}},
volume = {2019-March},
year = {2019}
}
@article{Rematas2016,
abstract = {Intercepting the GPU calls between the video game engine and the graphics card using RenderDoc [1] usually results in a massive amount of information, from texture and depth buffers to blend shape weights of the participating human models. While it is easy to locate and extract the color/depth buffers, the other information is more difficult to parse. For example, the naming of the intercepted variables is not always meaningful and in order to use them more sophisticated methods are needed [4]. In our case, the area of action (soccer field) has specific dimensions and structure that can be used to infer information such as camera matrices. In particular, our goal is to estimate the video game camera modelview matrix M mv and projection matrix M proj (OpenGL/DirectX), so we can invert the depth buffer that it is in Normalized Device Coordinates (NDC). Below (Appendix A) we describe the method to recover these matrices. The additional information that we need is a) a person segmentation of the texture buffer and b) an auxiliary camera that observes the field in world coordinates. To find which pixels belong to the players we use the semantic segmentation network of [7]. Then, for the auxiliary camera we follow the same approach as in Sec. 4.1: we estimate a camera with parameters M aux that observes the soccer field that lies in the world center with y axis 0. Note that the intrinsics and extrinsics of the auxiliary camera cannot be used directly for estimating M proj and M mv , since they do not include the near/far plane parameters and we do not know the video game's world system. Appendix A. Estimating Video Game Cameras To estimate the camera parameters (modelview and projection matrices) of a video game image, we need its corresponding NDC buffer, the auxiliary camera M aux that observes the field and the player segmentation mask. The modelview and projection matrices in OpenGL/DirectX are 4 × 4 and the parameters to be estimated are the rotation vector $\theta$ x , theta y , theta z , the translation vector t x , t y , t z and focal length, near and far plane f, z near , z far. Essentially we want to find the parameters that transformed the world scene to NDC so we can invert them: x clip y clip z clip w clip = M proj M mv x world y world z world 1 (1) xNDC yNDC zNDC = x clip /w clip y clip /w clip z clip /w clip (2) To estimate the OpenGL cameras we can rely on the structure of the scene: points that belong to the soccer field (and not to players) should lie on a plane with y equals to 0. The 3D world position of the ground pixels can be found by intersecting the rays from the auxiliary camera's ground pixels towards the ground. However, if we optimize only for the ground pixels, the z near , z far parameters collapse, resulting in wrong reconstructions for the players. In addition to the ground constraint, we can minimize the reprojection error of the player 3D points to the auxiliary camera. Therefore, for a sets of world 3D points X p with p ∈ ground and a set of 2D points (pixel location) y q with q ∈ player, the modelview matrix M mv and the projection matrix M proj are found by minimizing: min p∈ground (||X p − ˆ X p || 2) + $\lambda$ q∈player (||y q − ˆ y q || 2) (3) withˆX withˆ withˆX p = M −1 mv M −1 proj X NDC p , ˆ y q = M aux M −1 mv M −1 proj X NDC q X NDC p and X NDC q correspond to the NDC coordinates of ground points p and player points q. The NDC coordinates are found by dividing the the x and y pixel locations with image width and height respectively; the z coordinate is the value of the depth buffer at the specific pixel location. The weight $\lambda$ was set to 0.01. Appendix B. Implementation Details Training Data Our training data comes from the video game Electronic Arts FIFA 2016. The GPU calls between the game engine and the graphics card were obtained using RenderDoc v0.34. The playing teams were randomly selected and the camera was set to broadcast.},
author = {Rematas, Konstantinos and Kemelmacher-shlizerman, Ira and Curless, Brian and Seitz, Steve},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/soccer-tabletop-supplementary.pdf:pdf},
pages = {2016--2017},
title = {{Soccer on Your Tabletop Supplementary Material}},
url = {https://renderdoc.org.},
year = {2016}
}
@article{Le2016,
abstract = {We study the problem of smooth imitation learning for online sequence prediction, where the goal is to train a policy that can smoothly imitate demonstrated behavior in a dynamic and continuous environment in response to online, sequential context input. Since the mapping from context to behavior is often complex, we take a learning reduction approach to reduce smooth imitation learning to a regression problem using complex function classes that are regularized to ensure smoothness. We present a learning meta-algorithm that achieves fast and stable convergence to a good policy. Our approach enjoys several attractive properties, including being fully deterministic, employing an adaptive learning rate that can provably yield larger policy improvements compared to previous approaches, and the ability to ensure stable convergence. Our empirical results demonstrate significant performance gains over previous approaches.},
archivePrefix = {arXiv},
arxivId = {1606.00968},
author = {Le, Hoang M. and Kang, Andrew and Yue, Yisong and Carr, Peter},
eprint = {1606.00968},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/1606.00968.pdf:pdf},
title = {{Smooth Imitation Learning for Online Sequence Prediction}},
url = {http://arxiv.org/abs/1606.00968},
volume = {48},
year = {2016}
}
@article{Burke2019,
abstract = {DeepQB is a proposed application of deep neural networks to player tracking data from over two full seasons of American professional football. This novel approach demonstrates the ability to successfully understand complex aspects of the passing game, most notably quarterback decision-making. It can assess and compare individual quarterback pass target selection based on a snapshot presented to the passer by the receivers and defenders. Assessments of quarterback decision-making are made by comparing actual target selection to that predicted by our model. The model performs well, correctly identifying the targeted receiver in 60{\%} of cross-validated cases. When passers target the predicted receiver, passes are completed 74{\%} of the time, compared to 55{\%} when the QB targets any other receiver. This performance is surprisingly strong, given that the offense often conceals its intent by design, while defenses try not to allow any single receiver to be open. Further, quarterback passing skills separate and apart from his receivers and defense are isolated and assessed by comparing metrics of actual play success to the metrics of success predicted by the situation presented to the passer. This approach represents a new way for teams, media, and fans to understand and quantitatively assess quarterback decision-making, an aspect of the sport which has previously been opaque and inaccessible.},
author = {Burke, Brian and Analytics, Espn},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Burke, Analytics - 2019 - DeepQB Deep Learning with Player Tracking to Quantify Quarterback Decision-Making {\&}amp Performance.pdf:pdf},
pages = {1--13},
title = {{DeepQB: Deep Learning with Player Tracking to Quantify Quarterback Decision-Making {\&} Performance}},
url = {http://www.sloansportsconference.com/wp-content/uploads/2019/02/DeepQB.pdf},
year = {2019}
}
@article{Hadfield-menell2017,
author = {Hadfield-menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart and Dragan, Anca D},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/7253-inverse-reward-design.pdf:pdf},
number = {Nips},
title = {{Inverse Reward Design}},
year = {2017}
}
@article{Bialkowski2015,
abstract = {Although the collection of player and ball tracking data is fast becoming the norm in professional sports, large-scale mining of such spatiotemporal data has yet to surface. In this paper, given an entire season's worth of player and ball tracking data from a professional soccer league (≈400,000,000 data points), we present a method which can conduct both individual player and team analysis. Due to the dynamic, continuous and multi-player nature of team sports like soccer, a major issue is aligning player positions over time. We present a "role-based" representation that dynamically updates each player's relative role at each frame and demonstrate how this captures the short-term context to enable both individual player and team analysis. We discover role directly from data by utilizing a minimum entropy data partitioning method and show how this can be used to accurately detect and visualize formations, as well as analyze individual player behavior.},
author = {Bialkowski, Alina and Lucey, Patrick and Carr, Peter and Yue, Yisong and Sridharan, Sridha and Matthews, Iain},
doi = {10.1109/ICDM.2014.133},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Bialkowski et al. - 2015 - Large-Scale Analysis of Soccer Matches Using Spatiotemporal Tracking Data.pdf:pdf},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
keywords = {Formation,Role,Spatiotemporal Tracking Data,Sports Analytics},
number = {January},
pages = {725--730},
title = {{Large-Scale Analysis of Soccer Matches Using Spatiotemporal Tracking Data}},
volume = {2015-Janua},
year = {2015}
}
@article{Hoffer2017,
abstract = {Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance - known as the "generalization gap" phenomena. Identifying the origin of this gap and closing it had remained an open problem. Contributions: We examine the initial high learning rate training phase. We find that the weight distance from its initialization grows logarithmically with the number of weight updates. We therefore propose a "random walk on random landscape" statistical model which is known to exhibit similar "ultra-slow" diffusion behavior. Following this hypothesis we conducted experiments to show empirically that the "generalization gap" stems from the relatively small number of updates rather than the batch size, and can be completely eliminated by adapting the training regime used. We further investigate different techniques to train models in the large-batch regime and present a novel algorithm named "Ghost Batch Normalization" which enables significant decrease in the generalization gap without increasing the number of updates. To validate our findings we conduct several additional experiments on MNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices and beliefs concerning training of deep models and suggest they may not be optimal to achieve good generalization.},
archivePrefix = {arXiv},
arxivId = {1705.08741},
author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
eprint = {1705.08741},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Hoffer, Hubara, Soudry - 2017 - Train longer, generalize better closing the generalization gap in large batch training of neural network.pdf:pdf},
title = {{Train longer, generalize better: closing the generalization gap in large batch training of neural networks}},
url = {http://arxiv.org/abs/1705.08741},
year = {2017}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:pdf},
pages = {1--12},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@article{Agarwal2014,
author = {Agarwal, Alekh and Hsu, Daniel and Kale, Satyen and Langford, John and Schapire, Robert E},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Agarwal et al. - 2014 - Taming the Monster A Fast and Simple Algorithm for Contextual Bandits.pdf:pdf},
number = {i},
pages = {1--9},
title = {{Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits}},
url = {papers://d471b97a-e92c-44c2-8562-4efc271c8c1b/Paper/p996},
volume = {32},
year = {2014}
}
@article{Liu2018,
abstract = {Deep learning has started to have an impact on sports an-alytics. Several papers have applied action-value Q learning to quantify a team's chance of success, given the current match state. However, the black-box opacity of neural networks prohibits understanding why and when some actions are more valuable than others. This paper applies in-terpretable Mimic Learning to distill knowledge from the opaque neural net model to a transparent regression tree model. We apply Deep Reinforcement Learning to compute the Q function, and action impact under different game contexts, from 3M play-by-play events in the National Hockey League (NHL). The impact of an action is the change in Q-value due to the action. The play data along with the associated Q functions and impact are fitted by a mimic regression tree. We learn a general mimic regression tree for all players, and player-specific trees. The transparent tree structure facilitates understanding the general action values by feature influence and partial dependence plots, and player's exceptional characteristics by identifying player-specific relevant state regions.},
author = {Liu, Guiliang and Zhu, Wang and Schulte, Oliver},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/liu{\_}guiliang{\_}mlsa18.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
pages = {1--13},
title = {{Interpreting deep sports analytics: Valuing actions and players in the NHL}},
volume = {2284},
year = {2018}
}
@article{Hobbs2018,
author = {Hobbs, Jennifer and Power, Paul and Sha, Long and Ruiz, Hector and Lucey, Patrick},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Hobbs et al. - 2018 - Quantifying the Value of Transitions in Soccer via Spatiotemporal Trajectory Clustering.pdf:pdf},
pages = {1--11},
title = {{Quantifying the Value of Transitions in Soccer via Spatiotemporal Trajectory Clustering}},
year = {2018}
}
@article{Wang2018,
abstract = {During the 2017 NBA playoffs, Celtics coach Brad Stevens was faced with a difficult decision when defending against the Cavaliers: ``Do you double and risk giving up easy shots, or stay at home and do the best you can?"1 It's a tough call, but finding a good defensive strategy that effectively incorporates doubling can make all the difference in the NBA. In this paper, we analyze double teaming in the NBA, quantifying the trade-off between risk and reward. Using player trajectory data pertaining to over 643,000 possessions, we identified when the ball handler was double teamed. Given these data and the corresponding outcome (i.e., was the defense successful), we used deep reinforcement learning to estimate the quality of the defensive actions. We present qualitative and quantitative results summarizing our learned defensive strategy for defending. We show that our policy value estimates are predictive of points per possession and win percentage. Overall, the proposed framework represents a step toward a more comprehensive understanding of defensive strategies in the NBA. 1},
author = {Wang, Jiaxuan and Fox, Ian and Wiens, Jenna},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Wang, Fox, Wiens - 2018 - The advantage of doubling.pdf:pdf},
pages = {1--12},
title = {{The advantage of doubling}},
year = {2018}
}
@inproceedings{Power:2017:PCE:3097983.3098051,
address = {New York, NY, USA},
author = {Power, Paul and Ruiz, Hector and Wei, Xinyu and Lucey, Patrick},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/3097983.3098051},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/STATS{\_}DS{\_}passes-created-equally.pdf:pdf},
isbn = {978-1-4503-4887-4},
keywords = {clustering,passing,soccer,tracking data,unsupervised},
pages = {1605--1613},
publisher = {ACM},
series = {KDD '17},
title = {{Not All Passes Are Created Equal: Objectively Measuring the Risk and Reward of Passes in Soccer from Tracking Data}},
url = {http://doi.acm.org/10.1145/3097983.3098051},
year = {2017}
}
@article{Christiano2017,
abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
archivePrefix = {arXiv},
arxivId = {1706.03741},
author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
eprint = {1706.03741},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf:pdf},
number = {Nips},
title = {{Deep reinforcement learning from human preferences}},
url = {http://arxiv.org/abs/1706.03741},
year = {2017}
}
@article{Smith2017a,
abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01186v6},
author = {Smith, Leslie N.},
doi = {10.1109/WACV.2017.58},
eprint = {arXiv:1506.01186v6},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Smith - 2017 - Cyclical learning rates for training neural networks.pdf:pdf},
isbn = {9781509048229},
journal = {Proceedings - 2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017},
number = {April},
pages = {464--472},
title = {{Cyclical learning rates for training neural networks}},
year = {2017}
}
@article{Ross2011,
author = {Ross, St{\'{e}}phane and Gordon, Geoffrey J and Bagnell, J Andrew},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/ross11a.pdf:pdf},
journal = {In AISTATS},
pages = {627--635},
title = {{A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning}},
volume = {15},
year = {2011}
}
@article{Cervone2012,
abstract = {Basketball is a game of decisions; at any moment, a player can change the character of a possession by choosing to pass, dribble, or shoot. The current state of basketball analytics, however, provides no way to quantitatively evaluate the vast majority of decisions that players make, as most metrics are driven by events that occur at or near the end of a possession, such as points, turnovers, and assists. We propose a framework for using plater-tracking data to assign a point value to each moment of a possession by computing how many points the offense is expected to score by the end of the possession, a quantity we call expected possession value (EPV). EPV allows analysts to evaluate every decision made during a basketball game - whether it is to pass, dribble, or shoot - opening the door for a multitude of new metrics and analyses of basketball that quantify value in terms of points. In this paper, we propose a modeling framework for estimating EPV, present results of EPV computations performed using player-tracking data from the 2012-2013 season, adn provide several examples of EPV-derived metrics that answer real basketball questions.},
author = {Cervone, Dan and D'Amour, Alexander and Bornn, Luke and Goldsberry, Kirk},
doi = {http://dx.doi.org/10.1002/art.37735},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/81f4482586dbdd15fc92bee81ce68bcb6898.pdf:pdf},
isbn = {0004-3591},
issn = {0004-3591},
journal = {Arthritis and Rheumatism},
keywords = {Charlson Comorbidity Index,United Kingdom,body mass,cardiovascular disease,cohort analysis,college,data base,death,diabetes mellitus,diagnosis,disease modifying antirheumatic drug,drug use,follow up,hazard ratio,health,health practitioner,hospital,human,kidney disease,medical record,model,mortality,patient,population,population dynamics,predictive value,primary medical care,proportional hazards model,psoriasis,psoriatic arthritis,rheumatoid arthritis,rheumatology,risk,smoking,statistical significance},
pages = {S1081--S1082},
pmid = {71000555},
title = {{POINTWISE: Predicting Points and Valuing Decisions in Real Time with NBA Optical Tracking Data}},
url = {http://www.embase.com/search/results?subaction=viewrecord{\&}from=export{\&}id=L71000555{\%}0Ahttp://dx.doi.org/10.1002/art.37735{\%}0Ahttp://sfx.library.uu.nl/utrecht?sid=EMBASE{\&}issn=00043591{\&}id=doi:10.1002{\%}2Fart.37735{\&}atitle=Mortality+in+patients+with+psoriatic+art},
volume = {64},
year = {2012}
}
@article{Hadfield-Menell2016b,
abstract = {For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.},
archivePrefix = {arXiv},
arxivId = {1606.03137},
author = {Hadfield-Menell, Dylan and Dragan, Anca and Abbeel, Pieter and Russell, Stuart},
eprint = {1606.03137},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Hadfield-Menell et al. - 2016 - Cooperative Inverse Reinforcement Learning.pdf:pdf},
title = {{Cooperative Inverse Reinforcement Learning - ppt}},
url = {http://arxiv.org/abs/1606.03137},
year = {2016}
}
@article{Behnke2005a,
author = {Behnke, S and Bennewitz, M},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Behnke, Bennewitz - 2005 - Learning to play soccer using imitative reinforcement.pdf:pdf},
journal = {{\ldots}  Social Aspects of Robot Programming through  {\ldots}},
title = {{Learning to play soccer using imitative reinforcement}},
url = {http://nimbro.de/papers/ICRA05ws{\_}behnke{\_}bennewitz.pdf},
year = {2005}
}
@article{Abel2019a,
author = {Abel, David},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop//Beach, Abel - 2019 - ICML 2019 Notes.pdf:pdf},
number = {June},
pages = {1--87},
title = {{ICML 2019 Notes}},
year = {2019}
}
@article{Mehrasa2018,
abstract = {Team sports such as ice hockey and basketball involve complex player interactions. Modeling how players interact with each other presents a great challenge to researchers in the eield of sports analysis. The most common source of data available for this type of analysis is player trajectory tracking data, which encode vital information about the motion, action, and intention of players. At an individual level, each player exhibits a characteristic trajectory style that can distinguish him from other players. At a team level, a set of player trajectories forms unique dynamics that differentiate the team from others. We believe both players and teams possess their own particular spatio-temporal patterns hidden in the trajectory data and we propose a generic deep learning model that learns powerful representations from player trajectories. We show the effectiveness of our approach on event recognition and team classiiication.},
author = {Mehrasa, Nazanin and Zhong, Yatao and Tung, Frederick and Bornn, Luke and Mori, Greg},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop//Mehrasa et al. - 2018 - Deep Learning of Player Trajectory Representations for Team Activity Analysis.pdf:pdf},
pages = {1--8},
title = {{Deep Learning of Player Trajectory Representations for Team Activity Analysis}},
year = {2018}
}
@article{Ask2018,
abstract = {In imitation learning, behavior learning is generally done using the features extracted from the demonstration data. Recent deep learning algorithms enable the development of machine learning methods that can get high dimensional data as an input. In this work, we use imitation learning to teach the robot to dribble the ball to the goal. We use B-Human robot software to collect demonstration data and a deep convolutional network to represent the policies. We use top and bottom camera images of the robot as input and speed commands as outputs. The CNN policy learns the mapping between the series of images and speed commands. In 3D realistic robotics simulator experiments, we show that the robot is able to learn to search the ball and dribble the ball, but it struggles to align to the goal. The best-proposed policy model learns to score 4 goals out of 20 test episodes.},
archivePrefix = {arXiv},
arxivId = {1807.09205},
author = {Aşık, Okan and G{\"{o}}rer, Binnur and Akın, H. Levent},
eprint = {1807.09205},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Aşık, G{\"{o}}rer, Akın - 2018 - End-to-End Deep Imitation Learning Robot Soccer Case Study(2).pdf:pdf},
title = {{End-to-End Deep Imitation Learning: Robot Soccer Case Study}},
url = {http://arxiv.org/abs/1807.09205},
year = {2018}
}
@article{Hadfield-Menel2016,
abstract = {For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.},
author = {Hadfield-Menel},
doi = {10.1007/978-1-4899-7687-1_142},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Abbeel, Ng - 2019 - Inverse Reinforcement Learning.pdf:pdf},
journal = {Encyclopedia of Machine Learning and Data Mining},
number = {Nips},
pages = {678--682},
title = {{Cooperative Inverse Reinforcement Learning}},
year = {2016}
}
@book{Horton2018,
author = {Horton, Michael John},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Horton - 2018 - Algorithms for the Analysis of Spatio-Temporal Data From Team Sports.pdf:pdf},
isbn = {9783319180328},
number = {January},
title = {{Algorithms for the Analysis of Spatio-Temporal Data From Team Sports}},
url = {https://ses.library.usyd.edu.au/bitstream/2123/17755/2/Thesis - Michael Horton.pdf},
year = {2018}
}
@article{Alger2016,
abstract = {In this report, we describe " inverse reinforcement learning " . This is the problem of finding an unknown reward function for a Markov decision process (MDP), given an optimal policy for that MDP. We also describe existing methods to solve this problem, and investigate a pre-existing deep learning extension to these methods.},
author = {Alger, Maahew},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/irl.pdf:pdf},
pages = {1--19},
title = {{Deep Inverse Reinforcement Learning}},
url = {https://matthewja.com/pdfs/irl.pdf},
year = {2016}
}
@article{Wang2018a,
abstract = {This paper considers the problem of inverse reinforcement learning in zero-sum stochastic games when expert demonstrations are known to be not optimal. Compared to previous works that decouple agents in the game by assuming optimality in expert strategies, we introduce a new objective function that directly pits experts against Nash Equilibrium strategies, and we design an algorithm to solve for the reward function in the context of inverse reinforcement learning with deep neural networks as model approximations. In our setting the model and algorithm do not decouple by agent. In order to find Nash Equilibrium in large-scale games, we also propose an adversarial training algorithm for zero-sum stochastic games, and show the theoretical appeal of non-existence of local optima in its objective function. In our numerical experiments, we demonstrate that our Nash Equilibrium and inverse reinforcement learning algorithms address games that are not amenable to previous approaches using tabular representations. Moreover, with sub-optimal expert demonstrations our algorithms recover both reward functions and strategies with good quality.},
archivePrefix = {arXiv},
arxivId = {1801.02124},
author = {Wang, Xingyu and Klabjan, Diego},
eprint = {1801.02124},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Wang, Klabjan - 2018 - Competitive Multi-agent Inverse Reinforcement Learning with Sub-optimal Demonstrations.pdf:pdf},
title = {{Competitive Multi-agent Inverse Reinforcement Learning with Sub-optimal Demonstrations}},
url = {http://arxiv.org/abs/1801.02124},
year = {2018}
}
@article{Malik2018,
abstract = {Our goal is for AI systems to correctly identify and act according to their human user's objectives. Cooperative Inverse Reinforcement Learning (CIRL) formalizes this value alignment problem as a two-player game between a human and robot, in which only the human knows the parameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action space size exponential in the size of the reward parameter space. In this work, we exploit a specific property of CIRL---the human is a full information agent---to derive an optimality-preserving modification to the standard Bellman update; this reduces the complexity of the problem by an exponential factor and allows us to relax CIRL's assumption of human rationality. We apply this update to a variety of POMDP solvers and find that it enables us to scale CIRL to non-trivial problems, with larger reward parameter spaces, and larger action spaces for both robot and human. In solutions to these larger problems, the human exhibits pedagogic (teaching) behavior, while the robot interprets it as such and attains higher value for the human.},
archivePrefix = {arXiv},
arxivId = {1806.03820},
author = {Malik, Dhruv and Palaniappan, Malayandi and Fisac, Jaime F. and Hadfield-Menell, Dylan and Russell, Stuart and Dragan, Anca D.},
eprint = {1806.03820},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Malik et al. - 2018 - An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning.pdf:pdf},
title = {{An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning}},
url = {http://arxiv.org/abs/1806.03820},
year = {2018}
}
@article{Smith2017,
abstract = {In this paper, we describe a phenomenon, which we named "super-convergence", where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence. See http://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of super-convergence to win the DAWNBench challenge (see https://dawn.cs.stanford.edu/benchmark/).},
archivePrefix = {arXiv},
arxivId = {1708.07120},
author = {Smith, Leslie N. and Topin, Nicholay},
eprint = {1708.07120},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Smith, Topin - 2017 - Super-Convergence Very Fast Training of Neural Networks Using Large Learning Rates.pdf:pdf},
pages = {1--18},
title = {{Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates}},
url = {http://arxiv.org/abs/1708.07120},
year = {2017}
}
@article{Hadfield-Menell2016a,
abstract = {For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.},
archivePrefix = {arXiv},
arxivId = {1606.03137},
author = {Hadfield-Menell, Dylan and Dragan, Anca and Abbeel, Pieter and Russell, Stuart},
eprint = {1606.03137},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Hadfield-Menell et al. - 2016 - Cooperative Inverse Reinforcement Learning Supplementary Material.pdf:pdf},
pages = {1--5},
title = {{Cooperative Inverse Reinforcement Learning: Supplementary Material}},
url = {http://arxiv.org/abs/1606.03137},
year = {2016}
}
@article{Le2017,
abstract = {We study the problem of imitation learning from demonstrations of multiple coordinating agents. One key challenge in this setting is that learning a good model of coordination can be difficult, since coordination is often implicit in the demonstrations and must be inferred as a latent variable. We propose a joint approach that simultaneously learns a latent coordination model along with the individual policies. In particular, our method integrates unsupervised structure learning with conventional imitation learning. We illustrate the power of our approach on a difficult problem of learning multiple policies for fine-grained behavior modeling in team sports, where different players occupy different roles in the coordinated team strategy. We show that having a coordination model to infer the roles of players yields substantially improved imitation loss compared to conventional baselines.},
archivePrefix = {arXiv},
arxivId = {1703.03121},
author = {Le, Hoang M. and Yue, Yisong and Carr, Peter and Lucey, Patrick},
eprint = {1703.03121},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Le et al. - 2017 - Coordinated Multi-Agent Imitation Learning.pdf:pdf},
title = {{Coordinated Multi-Agent Imitation Learning}},
url = {http://arxiv.org/abs/1703.03121},
year = {2017}
}
@article{Amin2017,
abstract = {We introduce a novel repeated Inverse Reinforcement Learning problem: the agent has to act on behalf of a human in a sequence of tasks and wishes to minimize the number of tasks that it surprises the human by acting suboptimally with respect to how the human would have acted. Each time the human is surprised, the agent is provided a demonstration of the desired behavior by the human. We formalize this problem, including how the sequence of tasks is chosen, in a few different ways and provide some foundational results.},
archivePrefix = {arXiv},
arxivId = {1705.05427},
author = {Amin, Kareem and Jiang, Nan and Singh, Satinder},
eprint = {1705.05427},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/6778-repeated-inverse-reinforcement-learning.pdf:pdf},
number = {Nips},
pages = {1--10},
title = {{Repeated Inverse Reinforcement Learning}},
url = {http://arxiv.org/abs/1705.05427},
year = {2017}
}
@article{Cruz2017,
abstract = {Intelligent assistive robots have recently taken their rst steps toward entering domestic scenarios. It is thus expected that they perform tasks which are often considered rather simple for humans. However, for a robot to reach human-like performance diverse subtasks need to be accomplished in order to satisfactorily complete a given task. These subtasks include perception, understanding of the environment, learning strategies, knowledge representation, awareness of its own state, and manipulation of the environment. An open challenging issue is the time required by a robot to autonomously learn a new task. A strategy to speed up this apprenticeship period for autonomous robots is the integration of parent-like trainers to sca old the learning. In this regard, a trainer guides the robot to enhance the task performance in the same manner as caregivers may support infants in the accomplishment of a given task. In this thesis, we focus on these learning approaches, speci cally on interactive reinforcement learning to perform a domestic task. We use parent-like advice to explore two set-ups: agent-agent and human-agent interaction. First, we investigate agent-agent interactive reinforcement learning. We use an arti cial agent as a parent-like trainer. The arti cial agent is previously trained by autonomous reinforcement learning and afterward becomes the trainer of other agents. This interactive scenario allows us to experiment with the interplay of parameters like the probability of receiving feedback and the consistency of feedback. We show that the consistency of feedback deserves special attention since small variations on this parameter may considerably affect the learner's performance. Moreover, we introduce the concept of contextual affordances which allows to reduce the state-action space by avoiding failed-states, i.e., to avoid a group of states from which it is not possible to reach the goal-state of a task. By avoiding failed-states, the learner-agent is able to collect signi cantly more reward. The experiments also focus on the internal representation of knowledge in trainer-agents to improve the understanding of what the properties of a good teacher are. We show that using a polymath agent, i.e., an agent with more distributed knowledge among the states, it is possible to o er better advice to learner-agents compared to specialized agents. Thereafter, we study human-agent interactive reinforcement learning. Initially, experiments are performed with human parent-like advice using uni-modal speech guidance. The experimental set-up considers the use of di erent auditory sensors to compare how they a ect the consistency of advice and the learning performance. We observe that an impoverished speech recognition system may still help interactive reinforcement learning agents, although not to the same extent as in the ideal case of agent-agent interaction. Afterward, we perform an experiment including audiovisual parent-like advice. The set-up takes into account the integration of multi-modal cues in order to combine them into a single piece of consistent advice for the learner-agent. Additionally, we utilize contextual affordances to modulate the advice given to the robot to avoid failed-states and to effectively speed up the learning process. Multi-modal feedback produces more con dent levels of advice allowing learner-agents to bene t from this in order to obtain more reward and to gain it faster. This thesis contributes to knowledge in terms of studying the interplay of multimodal interactive feedback and contextual affordances. Overall, we investigate which parameters in uence the interactive reinforcement learning process and show that the apprenticeship of reinforcement learning agents can be sped up by means of interactive parent-like advice, multi-modal feedback, and a ordances-driven environmental models.},
author = {Cruz, Francisco},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Dissertation.pdf:pdf},
keywords = {Contextual affordances,Multi-modal integration,Reinforcement learning},
pages = {169},
title = {{Teaching Robots With Interactive Reinforcement Learning}},
year = {2017}
}
@article{Song2018,
abstract = {Imitation learning algorithms can be used to learn a policy from expert demonstrations without access to a reward signal. However, most existing approaches are not applicable in multi-agent settings due to the existence of multiple (Nash) equilibria and non-stationary environments. We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We further introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competing agents.},
archivePrefix = {arXiv},
arxivId = {1807.09936},
author = {Song, Jiaming and Ren, Hongyu and Sadigh, Dorsa and Ermon, Stefano},
eprint = {1807.09936},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Song et al. - 2018 - Multi-Agent Generative Adversarial Imitation Learning.pdf:pdf},
pages = {1--23},
title = {{Multi-Agent Generative Adversarial Imitation Learning}},
url = {http://arxiv.org/abs/1807.09936},
year = {2018}
}
@article{Hadfield-Menell2016,
abstract = {For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.},
archivePrefix = {arXiv},
arxivId = {1606.03137},
author = {Hadfield-Menell, Dylan and Dragan, Anca and Abbeel, Pieter and Russell, Stuart},
eprint = {1606.03137},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/1606.03137v2.pdf:pdf},
title = {{Cooperative Inverse Reinforcement Learning}},
url = {http://arxiv.org/abs/1606.03137},
year = {2016}
}
@article{Brown2018,
abstract = {Inverse reinforcement learning (IRL) infers a reward function from demonstrations, allowing for policy improvement and generalization. However, despite much recent interest in IRL, little work has been done to understand the minimum set of demonstrations needed to teach a specific sequential decision-making task. We formalize the problem of finding maximally informative demonstrations for IRL as a machine teaching problem where the goal is to find the minimum number of demonstrations needed to specify the reward equivalence class of the demonstrator. We extend previous work on algorithmic teaching for sequential decision-making tasks by showing a reduction to the set cover problem which enables an efficient approximation algorithm for determining the set of maximally-informative demonstrations. We apply our proposed machine teaching algorithm to two novel applications: providing a lower bound on the number of queries needed to learn a policy using active IRL and developing a novel IRL algorithm that can learn more efficiently from informative demonstrations than a standard IRL approach.},
archivePrefix = {arXiv},
arxivId = {1805.07687},
author = {Brown, Daniel S. and Niekum, Scott},
eprint = {1805.07687},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Brown, Niekum - 2018 - Machine Teaching for Inverse Reinforcement Learning Algorithms and Applications.pdf:pdf},
number = {2014},
title = {{Machine Teaching for Inverse Reinforcement Learning: Algorithms and Applications}},
url = {http://arxiv.org/abs/1805.07687},
year = {2018}
}
@article{Hausknecht2015,
abstract = {Recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces. However, to the best of our knowledge no previous work has succeeded at using deep neural networks in structured (parameterized) continuous action spaces. To fill this gap, this paper focuses on learning within the domain of simulated RoboCup soccer, which features a small set of discrete action types, each of which is parameterized with continuous variables. The best learned agent can score goals more reliably than the 2012 RoboCup champion agent. As such, this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space MDPs.},
archivePrefix = {arXiv},
arxivId = {1511.04143},
author = {Hausknecht, Matthew and Stone, Peter},
eprint = {1511.04143},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/1511.04143.pdf:pdf},
pages = {1--12},
title = {{Deep Reinforcement Learning in Parameterized Action Space}},
url = {http://arxiv.org/abs/1511.04143},
year = {2015}
}
@article{Mavrin2019,
abstract = {In distributional reinforcement learning (RL), the estimated distribution of value function models both the parametric and intrinsic uncertainties. We propose a novel and efficient exploration method for deep RL that has two components. The first is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus calculated from the upper quantiles of the learned distribution. In Atari 2600 games, our method outperforms QR-DQN in 12 out of 14 hard games (achieving 483 $\backslash${\%} average gain across 49 games in cumulative rewards over QR-DQN with a big win in Venture). We also compared our algorithm with QR-DQN in a challenging 3D driving simulator (CARLA). Results show that our algorithm achieves near-optimal safety rewards twice faster than QRDQN.},
archivePrefix = {arXiv},
arxivId = {1905.06125},
author = {Mavrin, Borislav and Zhang, Shangtong and Yao, Hengshuai and Kong, Linglong and Wu, Kaiwen and Yu, Yaoliang},
eprint = {1905.06125},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Mavrin et al. - 2019 - Distributional Reinforcement Learning for Efficient Exploration.pdf:pdf},
title = {{Distributional Reinforcement Learning for Efficient Exploration}},
url = {http://arxiv.org/abs/1905.06125},
year = {2019}
}
@article{SamuelL.SmithPieter-JanKindermans2007,
author = {{Samuel L. Smith, Pieter-Jan Kindermans}, Chris Ying {\&} Quoc V. Le},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Samuel L. Smith, Pieter-Jan Kindermans - 2007 - DON'T DECAY THE LEARNING RATE, INCREASE THE BATCH SIZE.pdf:pdf},
keywords = {dodd frank!!},
number = {2017},
pages = {1--11},
title = {{DON'T DECAY THE LEARNING RATE, INCREASE THE BATCH SIZE}},
year = {2007}
}
@article{Dinh2017,
abstract = {Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter {\&} Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.},
archivePrefix = {arXiv},
arxivId = {1703.04933},
author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
eprint = {1703.04933},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Dinh et al. - 2017 - Sharp Minima Can Generalize For Deep Nets.pdf:pdf},
title = {{Sharp Minima Can Generalize For Deep Nets}},
url = {http://arxiv.org/abs/1703.04933},
year = {2017}
}
@article{Le2017a,
abstract = {1 2017 Presented : Data- Driven Hoang . Le 1 , Peter 2 , Yisong 1 , and 3 California 1 , Disney 2 , and 3},
author = {Le, Hoang M. and Peter, Carr and Yue, Yisong},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop//Le, Peter, Yue - 2017 - Data-Driven Ghosting using Deep Imitation Learning.pdf:pdf},
journal = {MIT Sloan Sports Analytics Conference},
pages = {1--15},
title = {{Data-Driven Ghosting using Deep Imitation Learning}},
url = {https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20170228130457/Data-Driven-Ghosting-using-Deep-Imitation-Learning-Paper1.pdf},
year = {2017}
}
@article{Model2019,
author = {Model, Intention},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Model - 2019 - Policy Estimation based on Agoston ' s approach.pdf:pdf},
pages = {1--3},
title = {{Policy Estimation based on Agoston ' s approach}},
year = {2019}
}
@article{Rematas2018,
archivePrefix = {arXiv},
arxivId = {1806.00890},
author = {Rematas, Konstantinos and Kemelmacher-shlizerman, Ira and Curless, Brian and Seitz, Steve},
doi = {10.1109/CVPR.2018.00498},
eprint = {1806.00890},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/soccer{\_}on{\_}your{\_}tabletop.pdf:pdf},
title = {{Soccer on Your Tabletop Depth Depth Estimation Scene Reconstruction}},
year = {2018}
}
@article{Wiering1999,
abstract = {We use reinforcement learning (RL) to compute strategies for multiagent soccer teams. RL may profit significantly from world models (WMs) estimating state transition probabilities and rewards. In high-dimensional, continuous input spaces, however, learning accurate WMs is intractable. Here we show that incomplete WMs can help to quickly find good action selection policies. Our approach is based on a novel combination of CMACs and prioritized sweeping-like algorithms. Variants thereof outperform both Q(lambda)-learning with CMACs and the evolutionary method Probabilistic Incremental Program Evolution (PIPE) which performed best in previous comparisons.},
author = {Wiering, Marco and Sa{\l}ustowicz, Rafa{\l} and Schmidhuber, J{\"{u}}rgen},
doi = {10.1023/A:1008921914343},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop//Wiering, Sa{\l}ustowicz, Schmidhuber - 1999 - Reinforcement learning soccer teams with incomplete world models.pdf:pdf},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {cmac,evolutionary com-,q,reinforcement learning,simulated soccer,world models},
number = {1},
pages = {77--88},
title = {{Reinforcement learning soccer teams with incomplete world models}},
volume = {7},
year = {1999}
}
@article{Kaushik2016,
abstract = {Reinforcement Learning (RL) has been effectively used to solve complex problems given careful design of the problem and algorithm parameters. However standard RL approaches do not scale particularly well with the size of the problem and often require extensive engineering on the part of the designer to minimize the search space. To alleviate this problem, we present a model-free policy-based approach called Exploration from Demonstration (EfD) that uses human demonstrations to guide search space exploration. We use statistical measures of RL algorithms to provide feed- back to the user about the agent's uncertainty and use this to solicit targeted demonstrations useful from the agent's perspective. The demonstrations are used to learn an ex- ploration policy that actively guides the agent towards important aspects of the problem. We instantiate our approach in a gridworld and a popular arcade game and validate its performance under different experimental conditions. We show how EfD scales to large problems and provides convergence speed-ups over traditional exploration and interactive learning methods.},
author = {Kaushik, Subramanian and Isbell, Charles L. and Thomaz, Andrea L.},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Kaushik, Isbell, Thomaz - 2016 - Exploration from Demonstration for Interactive Reinforcement Learning Kaushik.pdf:pdf},
isbn = {9781450342391},
journal = {Proceedings of the 2016 International Conference on Autonomous Agents {\&} Multiagent Systems},
keywords = {active learning,exploration,human-agent interaction,reinforcement learning},
pages = {447--456},
title = {{Exploration from Demonstration for Interactive Reinforcement Learning Kaushik}},
url = {https://dl.acm.org/citation.cfm?id=2936990},
year = {2016}
}
@article{Sosic2018,
author = {{\v{S}}o{\v{s}}i{\'{c}}, Adrian and Sc, M},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/{\v{S}}o{\v{s}}i{\'{c}}, Sc - 2018 - Learning Models of Behavior From Demonstration and Through Interaction.pdf:pdf},
title = {{Learning Models of Behavior From Demonstration and Through Interaction}},
year = {2018}
}
@article{Hadfield-menell2010,
author = {Hadfield-menell, Dylan and Abbeel, Pieter},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Hadfield-menell, Abbeel - 2010 - Cooperative Inverse Reinforcement Learning.pdf:pdf},
title = {{Cooperative Inverse Reinforcement Learning - draft}},
year = {2010}
}
@article{Ross2010,
abstract = {Imitation Learning, while applied successfully on many large real-world$\backslash$nproblems, is typically addressed as a standard supervised learning$\backslash$nproblem, where it is assumed the training and testing data are i.i.d..$\backslash$nThis is not true in imitation learning as the learned policy influences$\backslash$nthe future test inputs (states) upon which it will be tested. We$\backslash$nshow that this leads to compounding errors and a regret bound that$\backslash$ngrows quadratically in the time horizon of the task. We propose two$\backslash$nalternative algorithms for imitation learning where training occurs$\backslash$nover several episodes of interaction. These two approaches share$\backslash$nin common that the learner's policy is slowly modified from executing$\backslash$nthe expert's policy to the learned policy. We show that this leads$\backslash$nto stronger performance guarantees and demonstrate the improved performance$\backslash$non two challenging problems: training a learner to play 1) a 3D racing$\backslash$ngame (Super Tux Kart) and 2) Mario Bros.; given input images from$\backslash$nthe games and corresponding actions taken by a human expert and near-optimal$\backslash$nplanner respectively.},
author = {Ross, St{\'{e}}phane and Bagnell, J Andrew},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Ross-AIStats10-paper.pdf:pdf},
issn = {15324435},
journal = {Aistats},
pages = {661--668},
title = {{Efficient Reductions for Imitation Learning}},
volume = {9},
year = {2010}
}
@article{DFitzgerald2011,
author = {DFitzgerald},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/DFitzgerald - 2011 - Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method.pdf:pdf},
pages = {1--41},
title = {{Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method}},
url = {papers3://publication/uuid/9B068F19-A3AD-43B0-BD3C-2219189F765D},
year = {2011}
}
@article{Nørstebø2016,
author = {N{\o}rsteb{\o}, Olav and Bjertnes, Vegard R{\o}dseth and Vabo, Eirik},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/N{\o}rsteb{\o}, Bjertnes, Vabo - 2016 - Valuing Individual Player Involvements in Norwegian Association Football Master's Thesis.pdf:pdf},
pages = {126},
title = {{Valuing Individual Player Involvements in Norwegian Association Football [Master's Thesis]}},
year = {2016}
}
@article{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf:pdf},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}
@article{Argall2009,
abstract = {We present a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Argall, Brenna D. and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
doi = {10.1016/j.robot.2008.10.024},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Argall et al. - 2009 - A survey of robot learning from demonstration.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Autonomous systems,Learning from demonstration,Machine learning,Robotics},
number = {5},
pages = {469--483},
title = {{A survey of robot learning from demonstration}},
volume = {57},
year = {2009}
}
@article{Arora2018,
abstract = {Inverse reinforcement learning is the problem of inferring the reward function of an observed agent, given its policy or behavior. Researchers perceive IRL both as a problem and as a class of methods. By categorically surveying the current literature in IRL, this article serves as a reference for researchers and practitioners in machine learning to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges which include accurate inference, generalizability, correctness of prior knowledge, and growth in solution complexity with problem size. The article elaborates how the current methods mitigate these challenges. We further discuss the extensions of traditional IRL methods: (i) inaccurate and incomplete perception, (ii) incomplete model, (iii) multiple rewards, and (iv) non-linear reward functions. This discussion concludes with some broad advances in the research area and currently open research questions.},
archivePrefix = {arXiv},
arxivId = {1806.06877},
author = {Arora, Saurabh and Doshi, Prashant},
eprint = {1806.06877},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Arora, Doshi - 2018 - A Survey of Inverse Reinforcement Learning Challenges, Methods and Progress.pdf:pdf},
keywords = {cs,edu,email addresses,function,generalization,imitation learning,inverse reinforcement learning,learning from demonstration,pdoshi,prashant,reinforcement learning,reward,sa08751,saurabh arora,uga},
title = {{A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress}},
url = {http://arxiv.org/abs/1806.06877},
year = {2018}
}
@article{Schaal1997,
author = {Schaal, Stefan},
doi = {10.1016/j.robot.2004.03.001},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Schaal - 1997 - Learning from demonstration.pdf:pdf},
isbn = {1558604863},
issn = {1049-5258},
journal = {Advances in neural information processing {\ldots}},
pages = {1040--1046},
pmid = {11540378},
title = {{Learning from demonstration}},
url = {http://reference.kfupm.edu.sa/content/l/e/learning{\_}from{\_}demonstration{\_}{\_}129612.pdf},
year = {1997}
}
@article{Mousavi2018a,
abstract = {In recent years, a specific machine learning method called deep learning has gained huge attraction, as it has obtained astonishing results in broad applications such as pattern recognition, speech recognition, computer vision, and natural language processing. Recent research has also been shown that deep learning techniques can be combined with reinforcement learning methods to learn useful representations for the problems with high dimensional raw data input. This chapter reviews the recent advances in deep reinforcement learning with a focus on the most used deep architectures such as autoencoders, convolutional neural networks and recurrent neural networks which have successfully been come together with the reinforcement learning framework.},
archivePrefix = {arXiv},
arxivId = {arXiv:1701.07274v6},
author = {Mousavi, Seyed Sajad and Schukat, Michael and Howley, Enda},
doi = {10.1007/978-3-319-56991-8_32},
eprint = {arXiv:1701.07274v6},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop//Mousavi, Schukat, Howley - 2018 - Deep Reinforcement Learning An Overview.pdf:pdf},
issn = {23673389},
journal = {Lecture Notes in Networks and Systems},
keywords = {Deep leaning,MDPs,Neural networks,Observable MDPs,Reinforcement learning},
pages = {426--440},
title = {{Deep Reinforcement Learning: An Overview}},
volume = {16},
year = {2018}
}
@article{Abbeel2004,
abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our. algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
author = {Abbeel, Pieter and Ng, Andrew Y.},
doi = {10.1145/1015330.1015430},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop//Microcentrifuge - 2011 - Sorvall Legend Micro 21R.pdf:pdf},
isbn = {1581138285},
journal = {Twenty-first international conference on Machine learning  - ICML '04},
number = {346},
pages = {1},
title = {{Apprenticeship learning via inverse reinforcement learning}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015430},
year = {2004}
}
