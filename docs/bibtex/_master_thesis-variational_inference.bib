Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Opper2009,
abstract = {Markov jump processes play an important role in a large number of application domains. However, realistic systems are analytically intractable and they have traditionally been analysed using simulation based techniques, which do not provide a framework for statistical inference. We propose a mean field approximation to perform posterior inference and parameter estimation. The approximation allows a practical solution to the inference problem, while still retaining a good degree of accuracy. We illustrate our approach on two biologically motivated systems.},
archivePrefix = {arXiv},
arxivId = {1905.05451},
author = {Opper, Manfred and Sanguinetti, Guido},
eprint = {1905.05451},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Opper, Sanguinetti - 2009 - Variational inference for Markov jump processes.pdf:pdf},
isbn = {160560352X},
journal = {Advances in Neural Information Processing Systems 20 - Proceedings of the 2007 Conference},
title = {{Variational inference for Markov jump processes}},
year = {2009}
}
@article{Blei2017a,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this article, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find a member of that family which is close to the target density. Closeness is measured by Kullbackâ€“Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this article is to catalyze statistical research on this class of algorithms. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {1601.00670},
author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
doi = {10.1080/01621459.2017.1285773},
eprint = {1601.00670},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Blei, Kucukelbir, McAuliffe - 2017 - Variational Inference A Review for Statisticians.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Algorithms,Computationally intensive methods,Statistical computing},
number = {518},
pages = {859--877},
title = {{Variational Inference: A Review for Statisticians}},
volume = {112},
year = {2017}
}
@article{Blei,
author = {Blei, David M},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/variational-inference-i.pdf:pdf},
pages = {1--12},
title = {{Variational-Inference-I.Pdf}}
}
