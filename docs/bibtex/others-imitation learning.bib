Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Argall2009,
abstract = {We present a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Argall, Brenna D. and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
doi = {10.1016/j.robot.2008.10.024},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Argall et al. - 2009 - A survey of robot learning from demonstration.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Autonomous systems,Learning from demonstration,Machine learning,Robotics},
number = {5},
pages = {469--483},
title = {{A survey of robot learning from demonstration}},
volume = {57},
year = {2009}
}
@article{Le2017a,
abstract = {1 2017 Presented : Data- Driven Hoang . Le 1 , Peter 2 , Yisong 1 , and 3 California 1 , Disney 2 , and 3},
author = {Le, Hoang M. and Peter, Carr and Yue, Yisong},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop//Le, Peter, Yue - 2017 - Data-Driven Ghosting using Deep Imitation Learning.pdf:pdf},
journal = {MIT Sloan Sports Analytics Conference},
pages = {1--15},
title = {{Data-Driven Ghosting using Deep Imitation Learning}},
url = {https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20170228130457/Data-Driven-Ghosting-using-Deep-Imitation-Learning-Paper1.pdf},
year = {2017}
}
@article{Le2016,
abstract = {We study the problem of smooth imitation learning for online sequence prediction, where the goal is to train a policy that can smoothly imitate demonstrated behavior in a dynamic and continuous environment in response to online, sequential context input. Since the mapping from context to behavior is often complex, we take a learning reduction approach to reduce smooth imitation learning to a regression problem using complex function classes that are regularized to ensure smoothness. We present a learning meta-algorithm that achieves fast and stable convergence to a good policy. Our approach enjoys several attractive properties, including being fully deterministic, employing an adaptive learning rate that can provably yield larger policy improvements compared to previous approaches, and the ability to ensure stable convergence. Our empirical results demonstrate significant performance gains over previous approaches.},
archivePrefix = {arXiv},
arxivId = {1606.00968},
author = {Le, Hoang M. and Kang, Andrew and Yue, Yisong and Carr, Peter},
eprint = {1606.00968},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/1606.00968.pdf:pdf},
title = {{Smooth Imitation Learning for Online Sequence Prediction}},
url = {http://arxiv.org/abs/1606.00968},
volume = {48},
year = {2016}
}
@article{Ask2018,
abstract = {In imitation learning, behavior learning is generally done using the features extracted from the demonstration data. Recent deep learning algorithms enable the development of machine learning methods that can get high dimensional data as an input. In this work, we use imitation learning to teach the robot to dribble the ball to the goal. We use B-Human robot software to collect demonstration data and a deep convolutional network to represent the policies. We use top and bottom camera images of the robot as input and speed commands as outputs. The CNN policy learns the mapping between the series of images and speed commands. In 3D realistic robotics simulator experiments, we show that the robot is able to learn to search the ball and dribble the ball, but it struggles to align to the goal. The best-proposed policy model learns to score 4 goals out of 20 test episodes.},
archivePrefix = {arXiv},
arxivId = {1807.09205},
author = {Aşık, Okan and G{\"{o}}rer, Binnur and Akın, H. Levent},
eprint = {1807.09205},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Aşık, G{\"{o}}rer, Akın - 2018 - End-to-End Deep Imitation Learning Robot Soccer Case Study(2).pdf:pdf},
title = {{End-to-End Deep Imitation Learning: Robot Soccer Case Study}},
url = {http://arxiv.org/abs/1807.09205},
year = {2018}
}
@article{Schaal1997,
author = {Schaal, Stefan},
doi = {10.1016/j.robot.2004.03.001},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Schaal - 1997 - Learning from demonstration.pdf:pdf},
isbn = {1558604863},
issn = {1049-5258},
journal = {Advances in neural information processing {\ldots}},
pages = {1040--1046},
pmid = {11540378},
title = {{Learning from demonstration}},
url = {http://reference.kfupm.edu.sa/content/l/e/learning{\_}from{\_}demonstration{\_}{\_}129612.pdf},
year = {1997}
}
@article{Ross2010,
abstract = {Imitation Learning, while applied successfully on many large real-world$\backslash$nproblems, is typically addressed as a standard supervised learning$\backslash$nproblem, where it is assumed the training and testing data are i.i.d..$\backslash$nThis is not true in imitation learning as the learned policy influences$\backslash$nthe future test inputs (states) upon which it will be tested. We$\backslash$nshow that this leads to compounding errors and a regret bound that$\backslash$ngrows quadratically in the time horizon of the task. We propose two$\backslash$nalternative algorithms for imitation learning where training occurs$\backslash$nover several episodes of interaction. These two approaches share$\backslash$nin common that the learner's policy is slowly modified from executing$\backslash$nthe expert's policy to the learned policy. We show that this leads$\backslash$nto stronger performance guarantees and demonstrate the improved performance$\backslash$non two challenging problems: training a learner to play 1) a 3D racing$\backslash$ngame (Super Tux Kart) and 2) Mario Bros.; given input images from$\backslash$nthe games and corresponding actions taken by a human expert and near-optimal$\backslash$nplanner respectively.},
author = {Ross, St{\'{e}}phane and Bagnell, J Andrew},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Ross-AIStats10-paper.pdf:pdf},
issn = {15324435},
journal = {Aistats},
pages = {661--668},
title = {{Efficient Reductions for Imitation Learning}},
volume = {9},
year = {2010}
}
@article{Ross2011,
author = {Ross, St{\'{e}}phane and Gordon, Geoffrey J and Bagnell, J Andrew},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/ross11a.pdf:pdf},
journal = {In AISTATS},
pages = {627--635},
title = {{A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning}},
volume = {15},
year = {2011}
}
@article{Kaushik2016,
abstract = {Reinforcement Learning (RL) has been effectively used to solve complex problems given careful design of the problem and algorithm parameters. However standard RL approaches do not scale particularly well with the size of the problem and often require extensive engineering on the part of the designer to minimize the search space. To alleviate this problem, we present a model-free policy-based approach called Exploration from Demonstration (EfD) that uses human demonstrations to guide search space exploration. We use statistical measures of RL algorithms to provide feed- back to the user about the agent's uncertainty and use this to solicit targeted demonstrations useful from the agent's perspective. The demonstrations are used to learn an ex- ploration policy that actively guides the agent towards important aspects of the problem. We instantiate our approach in a gridworld and a popular arcade game and validate its performance under different experimental conditions. We show how EfD scales to large problems and provides convergence speed-ups over traditional exploration and interactive learning methods.},
author = {Kaushik, Subramanian and Isbell, Charles L. and Thomaz, Andrea L.},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Kaushik, Isbell, Thomaz - 2016 - Exploration from Demonstration for Interactive Reinforcement Learning Kaushik.pdf:pdf},
isbn = {9781450342391},
journal = {Proceedings of the 2016 International Conference on Autonomous Agents {\&} Multiagent Systems},
keywords = {active learning,exploration,human-agent interaction,reinforcement learning},
pages = {447--456},
title = {{Exploration from Demonstration for Interactive Reinforcement Learning Kaushik}},
url = {https://dl.acm.org/citation.cfm?id=2936990},
year = {2016}
}
@article{Song2018,
abstract = {Imitation learning algorithms can be used to learn a policy from expert demonstrations without access to a reward signal. However, most existing approaches are not applicable in multi-agent settings due to the existence of multiple (Nash) equilibria and non-stationary environments. We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We further introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competing agents.},
archivePrefix = {arXiv},
arxivId = {1807.09936},
author = {Song, Jiaming and Ren, Hongyu and Sadigh, Dorsa and Ermon, Stefano},
eprint = {1807.09936},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Song et al. - 2018 - Multi-Agent Generative Adversarial Imitation Learning.pdf:pdf},
pages = {1--23},
title = {{Multi-Agent Generative Adversarial Imitation Learning}},
url = {http://arxiv.org/abs/1807.09936},
year = {2018}
}
@article{Le2017,
abstract = {We study the problem of imitation learning from demonstrations of multiple coordinating agents. One key challenge in this setting is that learning a good model of coordination can be difficult, since coordination is often implicit in the demonstrations and must be inferred as a latent variable. We propose a joint approach that simultaneously learns a latent coordination model along with the individual policies. In particular, our method integrates unsupervised structure learning with conventional imitation learning. We illustrate the power of our approach on a difficult problem of learning multiple policies for fine-grained behavior modeling in team sports, where different players occupy different roles in the coordinated team strategy. We show that having a coordination model to infer the roles of players yields substantially improved imitation loss compared to conventional baselines.},
archivePrefix = {arXiv},
arxivId = {1703.03121},
author = {Le, Hoang M. and Yue, Yisong and Carr, Peter and Lucey, Patrick},
eprint = {1703.03121},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Le et al. - 2017 - Coordinated Multi-Agent Imitation Learning.pdf:pdf},
title = {{Coordinated Multi-Agent Imitation Learning}},
url = {http://arxiv.org/abs/1703.03121},
year = {2017}
}
@article{Behnke2005a,
author = {Behnke, S and Bennewitz, M},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Behnke, Bennewitz - 2005 - Learning to play soccer using imitative reinforcement.pdf:pdf},
journal = {{\ldots}  Social Aspects of Robot Programming through  {\ldots}},
title = {{Learning to play soccer using imitative reinforcement}},
url = {http://nimbro.de/papers/ICRA05ws{\_}behnke{\_}bennewitz.pdf},
year = {2005}
}
