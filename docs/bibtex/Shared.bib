Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Abel2019,
abstract = {This document contains notes I took during the events I managed to make it to at ICML, in Long
Beach, CA, USA. Please feel free to distribute it and shoot me an email at david{\_}abel@brown.edu
if you find any typos or other items that need correcting.},
author = {Beach, Long and Abel, David},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop//Beach, Abel - 2019 - ICML 2019 Notes.pdf:pdf},
number = {June},
pages = {1--87},
title = {{ICML 2019 Notes}},
year = {2019}
}
@article{Bram2019,
abstract = {Sharing knowledge between tasks is vital for efficient learning in a multi-task setting. However, most research so far has focused on the easier case where knowledge transfer is not harmful, i.e., where knowledge from one task cannot negatively impact the performance on another task. In contrast, we present an approach to multi-task deep reinforcement learning based on attention that does not require any a-priori assumptions about the relationships between tasks. Our attention network automatically groups task knowledge into sub-networks on a state level granularity. It thereby achieves positive knowledge transfer if possible, and avoids negative transfer in cases where tasks interfere. We test our algorithm against two state-of-the-art multi-task/transfer learning approaches and show comparable or superior performance while requiring fewer network parameters.},
archivePrefix = {arXiv},
arxivId = {1907.02874},
author = {Bram, Timo and Brunner, Gino and Richter, Oliver and Wattenhofer, Roger},
eprint = {1907.02874},
month = {jul},
title = {{Attentive Multi-Task Deep Reinforcement Learning}},
url = {https://arxiv.org/abs/1907.02874 http://arxiv.org/abs/1907.02874},
year = {2019}
}
@article{Mousavi2018,
abstract = {In recent years, a specific machine learning method called deep learning has gained huge attraction, as it has obtained astonishing results in broad applications such as pattern recognition, speech recognition, computer vision, and natural language processing. Recent research has also been shown that deep learning techniques can be combined with reinforcement learning methods to learn useful representations for the problems with high dimensional raw data input. This chapter reviews the recent advances in deep reinforcement learning with a focus on the most used deep architectures such as autoencoders, convolutional neural networks and recurrent neural networks which have successfully been come together with the reinforcement learning framework.},
archivePrefix = {arXiv},
arxivId = {arXiv:1701.07274v6},
author = {Mousavi, Seyed Sajad and Schukat, Michael and Howley, Enda},
doi = {10.1007/978-3-319-56991-8_32},
eprint = {arXiv:1701.07274v6},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop//Mousavi, Schukat, Howley - 2018 - Deep Reinforcement Learning An Overview.pdf:pdf},
issn = {23673389},
journal = {Lecture Notes in Networks and Systems},
keywords = {Deep leaning,MDPs,Neural networks,Observable MDPs,Reinforcement learning},
pages = {426--440},
title = {{Deep Reinforcement Learning: An Overview}},
volume = {16},
year = {2018}
}
