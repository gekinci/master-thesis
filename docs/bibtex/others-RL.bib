Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Duan2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.02779v2},
author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter and Science, Computer},
eprint = {arXiv:1611.02779v2},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Duan et al. - 2017 - R EINFORCEMENT L EARNING.pdf:pdf},
pages = {1--14},
title = {{RL2: FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING}},
year = {2017}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:pdf},
pages = {1--12},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@article{Agarwal2014,
author = {Agarwal, Alekh and Hsu, Daniel and Kale, Satyen and Langford, John and Schapire, Robert E},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Agarwal et al. - 2014 - Taming the Monster A Fast and Simple Algorithm for Contextual Bandits.pdf:pdf},
number = {i},
pages = {1--9},
title = {{Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits}},
url = {papers://d471b97a-e92c-44c2-8562-4efc271c8c1b/Paper/p996},
volume = {32},
year = {2014}
}
@article{Abel2019a,
author = {Abel, David},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop//Beach, Abel - 2019 - ICML 2019 Notes.pdf:pdf},
number = {June},
pages = {1--87},
title = {{ICML 2019 Notes}},
year = {2019}
}
@article{Mavrin2019,
abstract = {In distributional reinforcement learning (RL), the estimated distribution of value function models both the parametric and intrinsic uncertainties. We propose a novel and efficient exploration method for deep RL that has two components. The first is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus calculated from the upper quantiles of the learned distribution. In Atari 2600 games, our method outperforms QR-DQN in 12 out of 14 hard games (achieving 483 $\backslash${\%} average gain across 49 games in cumulative rewards over QR-DQN with a big win in Venture). We also compared our algorithm with QR-DQN in a challenging 3D driving simulator (CARLA). Results show that our algorithm achieves near-optimal safety rewards twice faster than QRDQN.},
archivePrefix = {arXiv},
arxivId = {1905.06125},
author = {Mavrin, Borislav and Zhang, Shangtong and Yao, Hengshuai and Kong, Linglong and Wu, Kaiwen and Yu, Yaoliang},
eprint = {1905.06125},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Mavrin et al. - 2019 - Distributional Reinforcement Learning for Efficient Exploration.pdf:pdf},
title = {{Distributional Reinforcement Learning for Efficient Exploration}},
url = {http://arxiv.org/abs/1905.06125},
year = {2019}
}
@article{DFitzgerald2011,
author = {DFitzgerald},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/DFitzgerald - 2011 - Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method.pdf:pdf},
pages = {1--41},
title = {{Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method}},
url = {papers3://publication/uuid/9B068F19-A3AD-43B0-BD3C-2219189F765D},
year = {2011}
}
@article{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf:pdf},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}
@article{Mousavi2018a,
abstract = {In recent years, a specific machine learning method called deep learning has gained huge attraction, as it has obtained astonishing results in broad applications such as pattern recognition, speech recognition, computer vision, and natural language processing. Recent research has also been shown that deep learning techniques can be combined with reinforcement learning methods to learn useful representations for the problems with high dimensional raw data input. This chapter reviews the recent advances in deep reinforcement learning with a focus on the most used deep architectures such as autoencoders, convolutional neural networks and recurrent neural networks which have successfully been come together with the reinforcement learning framework.},
archivePrefix = {arXiv},
arxivId = {arXiv:1701.07274v6},
author = {Mousavi, Seyed Sajad and Schukat, Michael and Howley, Enda},
doi = {10.1007/978-3-319-56991-8_32},
eprint = {arXiv:1701.07274v6},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop//Mousavi, Schukat, Howley - 2018 - Deep Reinforcement Learning An Overview.pdf:pdf},
issn = {23673389},
journal = {Lecture Notes in Networks and Systems},
keywords = {Deep leaning,MDPs,Neural networks,Observable MDPs,Reinforcement learning},
pages = {426--440},
title = {{Deep Reinforcement Learning: An Overview}},
volume = {16},
year = {2018}
}
