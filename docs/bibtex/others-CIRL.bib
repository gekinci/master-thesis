Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Amin2017,
abstract = {We introduce a novel repeated Inverse Reinforcement Learning problem: the agent has to act on behalf of a human in a sequence of tasks and wishes to minimize the number of tasks that it surprises the human by acting suboptimally with respect to how the human would have acted. Each time the human is surprised, the agent is provided a demonstration of the desired behavior by the human. We formalize this problem, including how the sequence of tasks is chosen, in a few different ways and provide some foundational results.},
archivePrefix = {arXiv},
arxivId = {1705.05427},
author = {Amin, Kareem and Jiang, Nan and Singh, Satinder},
eprint = {1705.05427},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/6778-repeated-inverse-reinforcement-learning.pdf:pdf},
number = {Nips},
pages = {1--10},
title = {{Repeated Inverse Reinforcement Learning}},
url = {http://arxiv.org/abs/1705.05427},
year = {2017}
}
@article{Hadfield-Menell2016b,
abstract = {For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.},
archivePrefix = {arXiv},
arxivId = {1606.03137},
author = {Hadfield-Menell, Dylan and Dragan, Anca and Abbeel, Pieter and Russell, Stuart},
eprint = {1606.03137},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Hadfield-Menell et al. - 2016 - Cooperative Inverse Reinforcement Learning.pdf:pdf},
title = {{Cooperative Inverse Reinforcement Learning - ppt}},
url = {http://arxiv.org/abs/1606.03137},
year = {2016}
}
@article{Christiano2017,
abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
archivePrefix = {arXiv},
arxivId = {1706.03741},
author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
eprint = {1706.03741},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf:pdf},
number = {Nips},
title = {{Deep reinforcement learning from human preferences}},
url = {http://arxiv.org/abs/1706.03741},
year = {2017}
}
@article{Hadfield-menell2010,
author = {Hadfield-menell, Dylan and Abbeel, Pieter},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Hadfield-menell, Abbeel - 2010 - Cooperative Inverse Reinforcement Learning.pdf:pdf},
title = {{Cooperative Inverse Reinforcement Learning - draft}},
year = {2010}
}
@article{Malik2018,
abstract = {Our goal is for AI systems to correctly identify and act according to their human user's objectives. Cooperative Inverse Reinforcement Learning (CIRL) formalizes this value alignment problem as a two-player game between a human and robot, in which only the human knows the parameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action space size exponential in the size of the reward parameter space. In this work, we exploit a specific property of CIRL---the human is a full information agent---to derive an optimality-preserving modification to the standard Bellman update; this reduces the complexity of the problem by an exponential factor and allows us to relax CIRL's assumption of human rationality. We apply this update to a variety of POMDP solvers and find that it enables us to scale CIRL to non-trivial problems, with larger reward parameter spaces, and larger action spaces for both robot and human. In solutions to these larger problems, the human exhibits pedagogic (teaching) behavior, while the robot interprets it as such and attains higher value for the human.},
archivePrefix = {arXiv},
arxivId = {1806.03820},
author = {Malik, Dhruv and Palaniappan, Malayandi and Fisac, Jaime F. and Hadfield-Menell, Dylan and Russell, Stuart and Dragan, Anca D.},
eprint = {1806.03820},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Malik et al. - 2018 - An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning.pdf:pdf},
title = {{An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning}},
url = {http://arxiv.org/abs/1806.03820},
year = {2018}
}
@article{Chan2019,
abstract = {Learning preferences implicit in the choices humans make is a well studied problem in both economics and computer science. However, most work makes the assumption that humans are acting (noisily) optimally with respect to their preferences. Such approaches can fail when people are themselves learning about what they want. In this work, we introduce the assistive multi-armed bandit, where a robot assists a human playing a bandit task to maximize cumulative reward. In this problem, the human does not know the reward function but can learn it through the rewards received from arm pulls; the robot only observes which arms the human pulls but not the reward associated with each pull. We offer sufficient and necessary conditions for successfully assisting the human in this framework. Surprisingly, better human performance in isolation does not necessarily lead to better performance when assisted by the robot: a human policy can do better by effectively communicating its observed rewards to the robot. We conduct proof-of-concept experiments that support these results. We see this work as contributing towards a theory behind algorithms for human-robot interaction.},
archivePrefix = {arXiv},
arxivId = {arXiv:1901.08654v1},
author = {Chan, Lawrence and Hadfield-Menell, Dylan and Srinivasa, Siddhartha and Dragan, Anca},
doi = {10.1109/HRI.2019.8673234},
eprint = {arXiv:1901.08654v1},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/1901.08654.pdf:pdf},
isbn = {9781538685556},
issn = {21672148},
journal = {ACM/IEEE International Conference on Human-Robot Interaction},
keywords = {assistive agents,preference learning},
number = {Figure 1},
pages = {354--363},
title = {{The Assistive Multi-Armed Bandit}},
volume = {2019-March},
year = {2019}
}
@article{Hadfield-Menel2016,
abstract = {For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.},
author = {Hadfield-Menel},
doi = {10.1007/978-1-4899-7687-1_142},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Abbeel, Ng - 2019 - Inverse Reinforcement Learning.pdf:pdf},
journal = {Encyclopedia of Machine Learning and Data Mining},
number = {Nips},
pages = {678--682},
title = {{Cooperative Inverse Reinforcement Learning}},
year = {2016}
}
@article{Hadfield-Menell2016a,
abstract = {For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.},
archivePrefix = {arXiv},
arxivId = {1606.03137},
author = {Hadfield-Menell, Dylan and Dragan, Anca and Abbeel, Pieter and Russell, Stuart},
eprint = {1606.03137},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/Hadfield-Menell et al. - 2016 - Cooperative Inverse Reinforcement Learning Supplementary Material.pdf:pdf},
pages = {1--5},
title = {{Cooperative Inverse Reinforcement Learning: Supplementary Material}},
url = {http://arxiv.org/abs/1606.03137},
year = {2016}
}
@article{Hadfield-menell2017,
author = {Hadfield-menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart and Dragan, Anca D},
file = {:C$\backslash$:/Users/gizem/Documents/Mendeley Desktop/7253-inverse-reward-design.pdf:pdf},
number = {Nips},
title = {{Inverse Reward Design}},
year = {2017}
}
