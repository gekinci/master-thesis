\documentclass[]{article}
\usepackage[a4paper, total={6in, 9in}]{geometry}
\usepackage{amsmath}
\usepackage{algorithmic}
\usepackage[ruled, lined, longend]{algorithm2e}
\SetKwProg{Init}{Initialize}{}{}
%opening
\title{Incomplete communication models}
\author{Gizem Ekinci, Anam Tahir, Dominik Linzner}

\begin{document}
	
	\maketitle
	
	\section{Motivation}
	
	The state of a cell can be expressed in terms of its gene activation state $ X(t) \in \{0, 1\}^{N} $, e.g.
	\begin{center}
		\begin{math}
		X(t)=\left(\begin{array}{c}
		{0} \\
		{1} \\
		{\vdots} \\
		{0} \\
		{1}
		\end{array}\right)
		\end{math}\\
	\end{center}
	which draws a discrete valued trajectory in time $ X^{[0,T]} $. Based on this gene state, the cell may emit some message $ m_{X}(t) = \phi_{X}(X(t)) $. Let $ Z(t) $ denote some cell receiving the messages emitted by cells X, Y, then it may evolve some translation of the message $ c(t) = \phi _{Z} (m_{X}(t),m_{Y}(t)) = \phi_{Z}(\phi_{X}(X(t)), \phi_{Y}(X(t))) \equiv \psi(X(t), Y(t)) $. Given some task that Z may have to achieve in coordination with X, Y, this defines a POMDP problem. In the following we assume that the behaviour of Z has been shaped by evolution (close) to optimality. Our goal is now to infer the inter-cell language model from demonstrations. \\
	
	\section{Problem Definition}
	
	\subsection{Time-homogenous continuous-time Markov Processes}
	The messages that are emitted by the cells are modelled as independent time-homogeneous continuous-time Markov processes (CTMP). These processes are defined by transition intensity matrices $ Q_{X} $ and $ Q_{Y} $, whose intensities does not depend on time. In this matrix, the intensity $ q_{i} $ represents the instantaneous probability of leaving state i and $ q_{i,j} $ represents the instantaneous probability of switching from state $ i $ to $ j $.\\
	
	Infinitesimal transition probabilities in terms of the entries of transition matrices $ q_{ij} $ can be written as \cite{Cohn2010a}:
	\begin{equation}
	p_{i,j}(h)=\delta_{ij}+q_{i,j} h+o(h)
	\end{equation}
	where $ p_{i, j}(t) \equiv Pr(X^{(t+s)}=j\mid X^{(s)}=i) $ are Markov transition functions and o(.) is a function decaying to zero faster than its argument.\\
	
	The \textit{forward} or \textit{master equation} is then derived as follows:
	
	\begin{equation}
	p_{j}(t)=\sum_{\forall i} p_{i, j}(h) p_{i}(t-h)
	\end{equation}
	
	\begin{equation}
	\begin{split}
	\lim_{h\rightarrow 0} p_{j}(t) & = \lim_{h\rightarrow 0} \sum_{\forall i} \left[ \delta_{ij}+q_{i,j} h+o(h)\right]  p_{i}(t-h) \\ & = \lim_{h\rightarrow 0} p_{j}(t-h) + \lim_{h\rightarrow 0} h \sum_{\forall i} q_{i,j} p_{i}(t-h)
	\end{split}
	\end{equation}
	
	\begin{equation}
	\lim_{h\rightarrow 0} \frac{p_{j}(t) - p_{j}(t-h)}{h} = \lim_{h\rightarrow 0} \sum_{\forall i} q_{i,j} p_{i}(t-h)
	\end{equation}
	
	\begin{equation}
	\begin{split}
	\frac{d}{dt} p_{j}(t) & = \sum_{\forall i} q_{i,j} p_{i}(t) \\ & = \sum_{\forall i \neq j}\left[  q_{i,j} p_{i}(t) - q_{j,i} p_{j}(t) \right]
	\end{split}
	\label{eq:ME}
	\end{equation}
	Eq.\ref{eq:ME} can be written in matrix form:
	\begin{equation}
	\frac{d}{dt} p = p\textbf{Q}
	\end{equation}
	The solution to ODE, the time-dependent probability distribution $ p(t) $ is, 
	\begin{equation}
	p(t)=p(0) \exp (t\textbf{Q})
	\end{equation}
	with initial distribution $ p(0) $.\\
	
	The amount of time staying in a state i is exponentially distributed with parameter $ q_{i} $. The probability density function f for staying in the state $ i $ is:
	\begin{equation}
	f(t)=q_{i} \exp \left(-q_{i} t\right)
	\label{eq:f(t)_homo}
	\end{equation}
	
	To write down the likelihood of a trajectory sampled from a homogenous CTMC $ X $, first let us consider one transition $ d = <i,j,t> $, where transition happens after time t from state i to j. The likelihood of this transition is:
	\begin{equation}
	L_{X}(d \mid \textbf{Q})=\left(q_{i} \exp \left(-q_{i} t\right)\right)\left(\frac{q_{i,j}}{q_{i}}\right)
	\end{equation}
	By defining sufficient statistics over out dataset $ D $ as $ T[i] $, the amount of time spent in state i, $ M[i,j] $ the number of transitions from state i to j, we can write down the likelihood of a trajectory $  X^{\left[0,T\right] } $,
	\begin{equation}
	\begin{split}
	L_{X}(D \mid \textbf{Q}) &=  \prod_{d \in D} L(d \mid \textbf{Q}) \\&=\left(\prod_{\forall i} q_{i}^{M[i]} \exp \left(-q_{i} T[i]\right)\right)\left(\prod_{\forall i} \prod_{\forall j \neq i} \left(\frac{q_{i,j}}{q_{i}}\right)^{M\left[i, j\right]}\right)
	\label{eq:lh_traj_homo}
	\end{split}
	\end{equation}
	
	\subsection{Time-inhomogeneous continuous-time Markov Processes}
	The entire communication is defined in the framework of continuous-time Bayesian network (CTBN). In an conventional CTBN, while every node is a Markov process itself, the leaf nodes are \textit{conditional} Markov processes, a type of inhomogeneous Markov process, where the intensities change over time, but not as a function of time rather as a function of parent states. \cite{Nodelman1995} \\
	
	For inhomogeneous Markov processes, Eq.\ref{eq:f(t)_homo} becomes:
	\begin{equation}
	f(t) = q_{i}(t) \exp \left(-\int_{s}^{l} q_{i}(u) d u\right)
	\end{equation}
	
	Let X be an inhomogeneous Markov process, and $  X^{\left[0,T\right] } $ is a trajectory sampled from this process. We define $ m $ number of transitions, with $ 0 = t_{0} < t_{1} < ... < t_{m} $ are the times where transition occurred, and $ x{0}, x_{1},...,x_{m} $ are the observed states. The likelihood of trajectory  $  X^{\left[0,T\right] } $ is  as follows: 
	\begin{equation}
	L= \prod_{k=1}^{m} \left[ q_{x_{k-1}, x_{k}} (t_{k}) \exp \left(-\int_{t_{k-1}}^{t_{k}} q_{x_{k-1}}(u) d u\right) \frac{q_{x_{k-1}, x_{k}} (t_{k})}{q_{x_{k-1}}(t_{k})}\right] 
	\label{eq:lh_traj_inhomo}
	\end{equation}
	In the paper \cite{Perez-Ocon2000}, this likelihood is written only considering the amount has been spent on one state, while the last part regarding the probability of the transition is ignored. On the other hand, in this paper \cite{Nodelman2014}, instead of integrating over time, $ T[i] $, in Eq.\ref{eq:lh_traj_homo}, is split to two parts $ T[x\mid u] = T_{r}[i\mid par(x)] + T_{c}[i\mid par(x)] $, where $ T_{r}  $ is the total durations that terminate with X remaining at state i, or end of trajectory, $ T_{c} $ is the total durations termination with X leaving state i. While the second derivation is consistent with Eq.\ref{eq:lh_traj_inhomo}, the first one is not.\\
	
	We can formulate our graphical model as $ S = [X, Y, Z] $, where $ Par(Z) = {X,Y} $ and $ Par(X) = Par(Y) =\emptyset $. We can then write the conditional probability:
	\begin{equation}
	\begin{split}
	P\left(S^{\left(t+h\right)}=s' | S^{\left(t\right)}=s\right) = & P\left(X^{\left( t+h\right) }=x' | X^{\left(t\right)}=x, Par(X)^{\left(t\right)}={y,z}\right) \\&  P\left(Y^{\left( t+h\right) }=y' | Y^{\left(t\right)}=y\right) P\left(Z^{\left( t+h\right) }=z' | ^{\left(t\right)}=z\right)
	\end{split} 
	\end{equation}
	
	\subsection{Partially observable Markov Decision Processes}
	In our problem, cell Z does not have a direct access to its parents' states, rather it observes a summary of them. This is different than observing the states through noisy binary channels, and presents a POMDP problem. Even though it is assumed that the policy of Z is optimal given the true states of parents, the problem of inferring the observational model remains unsolved and to be addressed.
	
	In a conventional POMDP, given the \textit{transition function}, $ T(s, a, s^{\prime})$  and \textit{observation function}, $ O(s^{\prime}, a, o) $, the belief state update is computed as follows \cite{Kaelbling2011} \footnote{Since for now, we assume there is no affect of Z's action on the observation or transition function, $ a $ is omitted from the equation.}:
	\begin{equation}
	\begin{aligned}
	b^{\prime}\left(s^{\prime}\right) &=\operatorname{Pr}\left(s^{\prime} | o, a, b\right) =\operatorname{Pr}\left(s^{\prime} | o, b\right) \\
	&=\frac{\operatorname{Pr}\left(o | s^{\prime}, b\right) \operatorname{Pr}\left(s^{\prime} | b\right)}{\operatorname{Pr}(o | b)} \\
	&=\frac{\operatorname{Pr}\left(o | s^{\prime}\right) \sum_{s \in \mathcal{S}} \operatorname{Pr}\left(s^{\prime} | b, s\right) \operatorname{Pr}(s | b)}{\operatorname{Pr}(o | b)} \\
	&=\frac{O\left(s^{\prime}, o\right) \sum_{s \in \mathcal{S}} T\left(s, s^{\prime}\right) b(s)}{\operatorname{Pr}(o | b)}
	\end{aligned}
	\label{eq:b}
	\end{equation}
	
	
	In Eq.\ref{eq:b}, the transition function is time-independent. However, in our setting, the parent nodes X and Y are modelled as CTMCs with time-dependent transition matrices. To derive \textit{continuous-time belief state}, $ b(t) $, first we need to get the joint transition matrix of X and Y, two independent processes. This operation called \textit{amalgamation} is described in detail by Nodelman \textit{et al} \cite{Nodelman1995}. Let us denote this matrix by $ Q_{XY} $. \\
	Now we can derive the belief state as follows: 
	
	\begin{equation}
	b_{s^{\prime}}(t) =\frac{\operatorname{Pr}\left(o |s^{\prime}\right) \sum_{s \in \mathcal{S}} \operatorname{Pr}\left(S(t)=s^{\prime} | S(t-h)=s\right) b_{s}(t-h)}{\operatorname{Pr}(o | b)}
	\end{equation}\\
	
	\begin{equation}
	\begin{aligned}
	\lim_{h\rightarrow 0} b_{s^{\prime}}(t) &= C \lim_{h\rightarrow 0}\left[  \sum_{s \in \mathcal{S}} p_{s, s^{\prime}}(t) b_{s}(t-h)\right]\\
	&= C \lim_{h\rightarrow 0}\left[  \sum_{s \in \mathcal{S}} \left[ \delta_{ss^{\prime}}+q_{s,s^{\prime}} h+o(h)\right]  b_{s}(t-h)\right]\\
	&= C \lim_{h\rightarrow 0}\left[ b_{s^{\prime}}(t-h) + \sum_{s \in \mathcal{S}} q_{s,s^{\prime}} h  b_{s}(t-h)\right]
	\end{aligned}
	\end{equation}
	Due to the scaling factor $ C =\frac{\operatorname{Pr}\left(o |s^{\prime}\right)}{\operatorname{Pr}(o | b)} $, cannot derive $ \frac{d}{dt} b = \textbf{Q}b$, .\\
	However, we can use the conditional distribution of Markov processes as the transition function $ T(s'|s) $ to derive the continuous-time belief state. The conditional distribution over the value of X, a homogenous Markov process, at time t given its value at an earlier time s can be written as 
	\begin{equation}
	\operatorname{Pr}\left(X(t) \mid X(s)\right) = \exp(\textbf{Q}(t-s))
	\end{equation}
	Then the continuous-time belief state becomes:
	\begin{equation}
	b_{s^{\prime}}(t) =\frac{\operatorname{Pr}\left(o |s^{\prime}\right) \sum_{s \in \mathcal{S}}\left[ \exp(\textbf{Q}(h))\right]_{s,s'}  b_{s}(t-h)}{\operatorname{Pr}(o | b)}
	\end{equation}\\
	
	\subsection{Simulation - Gillespie algorithm}
	\subsubsection{Generative CTBN}
	
	
	\begin{algorithm}
		\SetKwFunction{isOddNumber}{isOddNumber}
		\SetKwInOut{KwIn}{Input}
		\SetKwInOut{KwOut}{Output}
		\SetKwInOut{Init}{Initialize}
		
		\KwIn{Structure of the network with N local variables $ X_{1}, X_{2}, ..., X_{n} $ as a dict in the format of $ \left\lbrace X_{i}: [par(X_{i})]\right\rbrace $ with state-space $ S = \left\lbrace x_{0}, ..., x_{m} \right\rbrace $  \\ $ T_{max} $ to terminate simulation}
		\KwOut{Sample trajectory of the network}
		
		\Init{Initialize node values $ X_{i}^{0} = x_{j} \in S$\\Transition intensity matrices $ \textbf{Q}_{X_{i}} $, $ q_{jk} \in [0,1]$ for $ j\neq k $ }
		
		\While{$ t < T_{max} $}{
			$ \tau \sim \exp(\sum_{\forall i} \sum_{\forall j \neq k} \ q^{X_{i}}_{j,k}) $\\
			\textit{transitioning node is randomly drawn with probability}  $ P(X_{i}) = \frac{q^{X_{i}}_{i}}{\sum_{\forall i} q^{X_{i}}_{i}}$\\
			\textit{next state is randomly drawn with probability} $ P(x_{j}) = \frac{q^{X_{i}}_{i}}{\sum_{\forall i} q^{X_{i}}_{i}} $\\
			$ t \leftarrow t + \tau $
		}	
		
		\caption{Generative CTBN}
	\end{algorithm}
	
	\pagebreak
	\bibliographystyle{ieeetr}
	\bibliography{../bibtex/_master_thesis}
\end{document}
